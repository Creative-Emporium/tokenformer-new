[base]
encoding = gpt2
languages = en
heads = 12
context = 1024
embedding = 768
layers = 12
training = 1000; 100; 8

[transformer-gpt2]
use = plain/base
model = transformer

[tokenformer-gpt2]
use = plain/base
model = tokenformer
parameters = 1024

[transformer-gpt2-test]
use = plain/base
model = transformer
training = 10; 100; 1

[tokenformer-gpt2-test]
use = plain/base
model = tokenformer
training = 10; 100; 1